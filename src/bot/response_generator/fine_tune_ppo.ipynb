{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/.cache/pypoetry/virtualenvs/chat-bot-20tW9agt-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "from dataclasses import dataclass, Field\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from bitsandbytes.optim import PagedLion32bit\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "from peft.peft_model import PeftModel\n",
    "from tqdm.auto import tqdm, trange\n",
    "from transformers import (\n",
    "\tBitsAndBytesConfig,\n",
    "\tGenerationConfig,\n",
    "\tHfArgumentParser,\n",
    "\tpipeline, TextStreamer\n",
    ")\n",
    "from transformers.hf_argparser import HfArg\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "from libs import CommonScriptArguments, CommonWanDBArguments, ResponseGeneratorPipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (cache).\n",
      "Your token has been saved to /home/user/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/user/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This parameter should be any of Sentiment Analysis, Candidate Generator, Emotion Predictor, Emotion Model, Similarity Analysis, Response Generator, your input is Candidates Generator\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# @dataclass\n",
    "# class ScriptArguments(CommonScriptArguments):\n",
    "# \tchat_template_file: Field[str] = HfArg(aliases=\"--chat-template-file\", default=\"\")\n",
    "\n",
    "@dataclass\n",
    "class ScriptArguments(CommonScriptArguments):\n",
    "    chat_template_file: str = HfArg(aliases=\"--chat-template-file\", default=\"\")\n",
    "\n",
    "\n",
    "\n",
    "# config_getter = ArgumentParser()\n",
    "# config_getter.add_argument(\"--json_file\", required=True, type=str)\n",
    "# config = config_getter.parse_args()\n",
    "\n",
    "parser = HfArgumentParser((ScriptArguments, CommonWanDBArguments))\n",
    "args, wandb_args = parser.parse_json_file(\"/home/user/github/chat-bot/src/bot/response_generator/args/ppo_arg.json\")\n",
    "# args, wandb_args = parser.parse_json_file(config.json_file)\n",
    "\n",
    "chat_template: dict = eval(open(args.chat_template_file, \"r\", encoding=\"utf-8\", closefd=True).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myangyx30678\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/user/github/chat-bot/src/bot/response_generator/wandb/run-20240717_164538-d6exs5tz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yangyx30678/emotion-chat-bot-ncu/runs/d6exs5tz' target=\"_blank\">atomic-pine-133</a></strong> to <a href='https://wandb.ai/yangyx30678/emotion-chat-bot-ncu' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yangyx30678/emotion-chat-bot-ncu' target=\"_blank\">https://wandb.ai/yangyx30678/emotion-chat-bot-ncu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yangyx30678/emotion-chat-bot-ncu/runs/d6exs5tz' target=\"_blank\">https://wandb.ai/yangyx30678/emotion-chat-bot-ncu/runs/d6exs5tz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize Wandb\n",
    "run = wandb.init(\n",
    "\tjob_type=wandb_args.job_type,\n",
    "\tconfig=wandb_args.config,\n",
    "\tproject=wandb_args.project,\n",
    "\tgroup=wandb_args.group,\n",
    "\tnotes=wandb_args.notes,\n",
    "\tmode=wandb_args.mode,\n",
    "\tresume=wandb_args.resume\n",
    ")\n",
    "wandb.config[\"chat_template\"] = chat_template[\"template\"]\n",
    "wandb.config[\"instruction_template\"] = chat_template[\"instruction\"]\n",
    "wandb.config[\"response_template\"] = chat_template[\"response\"]\n",
    "wandb.config[\"special_tokens\"] = chat_template[\"special_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Dataset\n",
    "# dataset = load_dataset(\"hermeschen1116/daily_dialog_for_RG\", num_proc=16, trust_remote_code=True)\n",
    "# dataset = concatenate_datasets([dataset[\"train\"], dataset[\"validation\"]])\n",
    "# dataset = dataset.train_test_split(train_size=0.05)[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_length: int = 2 * wandb.config[\"num_turns_history\"]\n",
    "# dataset = dataset.filter(lambda sample: len(sample) >= (2 + history_length), input_columns=\"prompt\", num_proc=16)\n",
    "# print(f\"dataset size after filter: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.map(lambda sample: {\n",
    "# \t\"prompt\": sample[i: i + 2] for i in range(0, len(sample) - 2, 2)\n",
    "# }, input_columns=\"prompt\", batched=False, num_proc=16)\n",
    "\n",
    "# system_prompt: list = [{\"role\": \"system\", \"content\": {\"emotion\": \"\", \"dialog\": wandb.config[\"system_prompt\"]}}]\n",
    "\n",
    "# dataset = dataset.map(lambda samples: {\n",
    "# \t\"prompt\": [system_prompt + sample for sample in samples]\n",
    "# }, input_columns=\"prompt\", batched=True, num_proc=16)\n",
    "\n",
    "# emotion_labels: list = [\"neutral\", \"anger\", \"disgust\", \"fear\", \"happiness\", \"sadness\", \"surprise\"]\n",
    "\n",
    "# dataset = dataset.map(lambda samples: {\n",
    "# \t\"query\": [\n",
    "# \t\tsample[:-1] + [{\"role\": \"assistant\", \"content\": {\"emotion\": sample[-1][\"content\"][\"emotion\"], \"dialog\": \"\"}}]\n",
    "# \t\tfor sample in samples\n",
    "# \t],\n",
    "# \t\"label\": [emotion_labels.index(sample[-1][\"content\"][\"emotion\"]) for sample in samples]\n",
    "# }, input_columns=\"prompt\", remove_columns=\"prompt\", batched=True, num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth: Fast Llama patching release 2024.7\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 23.669 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.6. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = True]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32005, 4096)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Tokenizer\n",
    "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "\twandb.config[\"base_model\"],\n",
    "\tattn_implementation=\"flash_attention_2\",\n",
    "\tpretraining_tp=1,\n",
    "\tload_in_4bit=True,\n",
    "\tuse_cache=False,\n",
    "\tdevice_map=\"auto\",\n",
    "\tuse_gradient_checkpointing=True,\n",
    "\tlow_cpu_mem_usage=True,\n",
    "\ttrust_remote_code=True,\n",
    ")\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.clean_up_tokenization_spaces = True\n",
    "tokenizer.chat_template = wandb.config[\"chat_template\"]\n",
    "tokenizer.add_special_tokens(wandb.config[\"special_tokens\"])\n",
    "base_model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 262,184,960 || all params: 7,020,630,016 || trainable%: 3.7345\n"
     ]
    }
   ],
   "source": [
    "base_model_with_adapter = PeftModel.from_pretrained(base_model, wandb.config[\"adapter\"])\n",
    "model = torch.compile(base_model_with_adapter)\n",
    "FastLanguageModel.for_inference(model)\n",
    "base_model_with_adapter.print_trainable_parameters()\n",
    "FastLanguageModel.for_inference(base_model_with_adapter)\n",
    "\n",
    "# base_model_with_adapter = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "# \tbase_model_with_adapter,\n",
    "# \tdevice_map=\"auto\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = dataset.map(lambda sample: {\n",
    "# \t\"input_ids\": tokenizer.apply_chat_template(\n",
    "# \t\tsample,\n",
    "# \t\ttokenize=True,\n",
    "# \t\tpadding=\"max_length\",\n",
    "# \t\tmax_length=wandb.config[\"max_input_tokens\"],\n",
    "# \t\tadd_generation_prompt=True,\n",
    "# \t\treturn_tensors=\"pt\"\n",
    "# \t)[0]\n",
    "# }, input_columns=\"query\", num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sentiment Analysis\n",
    "# analyser = pipeline(\n",
    "# \tmodel=wandb.config[\"sentiment_analysis_model\"],\n",
    "# \ttokenizer=wandb.config[\"sentiment_analysis_model\"],\n",
    "# \tmax_length=512,\n",
    "# \ttruncation=True,\n",
    "# \tframework=\"pt\",\n",
    "# \ttask=\"sentiment-analysis\",\n",
    "# \tnum_workers=16,\n",
    "# \tdevice_map=\"auto\",\n",
    "# \ttorch_dtype=\"auto\",\n",
    "# \tmodel_kwargs={\n",
    "# \t\t\"quantization_config\": BitsAndBytesConfig(\n",
    "# \t\t\tload_in_4bit=True,\n",
    "# \t\t\tbnb_4bit_compute_dtype=torch.float16\n",
    "# \t\t),\n",
    "# \t\t\"id2label\": {k: v for k, v in enumerate(emotion_labels)},\n",
    "# \t\t\"label2id\": {v: k for k, v in enumerate(emotion_labels)},\n",
    "# \t\t\"low_cpu_mem_usage\": True\n",
    "# \t},\n",
    "# \ttrust_remote_code=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_analysis_model = torch.compile(analyser.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_length = 69\n",
    "# def calculate_emotion_score(response: str, correct_emotion: str) -> float:\n",
    "#     # correct: save the score from analyser \n",
    "#     # wrong: [TO-DO] (save 1 - score from analyser )\n",
    "#     emotion_output = analyser(response)[0]\n",
    "#     if emotion_output[\"label\"] == correct_emotion:\n",
    "#         emotion_score = emotion_output[\"score\"] * 10\n",
    "#     else:\n",
    "#         emotion_score = 1 - emotion_output[\"score\"]\n",
    "#     return emotion_score\n",
    "\n",
    "# def calculate_length_score(response: str) -> float:\n",
    "#     # use reciprocal of length difference to calculate\n",
    "#     # the larger the difference the smaller the score is\n",
    "#     length_diff = abs(len(response) - target_length)\n",
    "#     print(length_diff)\n",
    "#     length_score = 1 / (length_diff + 1)\n",
    "#     return length_score\n",
    "\n",
    "# def reward(batch: dict) -> list:\n",
    "#     print(\"Hello Huston, here is a reward function\")\n",
    "#     correct_emotion = batch['query'][2]['content']['emotion']\n",
    "#     print(correct_emotion)\n",
    "#     rewards = []\n",
    "#     for response in batch[\"response\"]:\n",
    "#         emotion_score = calculate_emotion_score(response, correct_emotion)\n",
    "#         length_score = calculate_length_score(response)\n",
    "#         # use the product of two score as reward\n",
    "#         reward_product = emotion_score * length_score\n",
    "#         rewards.append(reward_product)\n",
    "    \n",
    "#     return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock_batch = {\n",
    "#     \"query\": [\n",
    "#         {\"content\": {\"text\": \"Hello, how are you?\", \"emotion\": \"sadness\"}},\n",
    "#         {\"content\": {\"text\": \"It's a nice day today.\", \"emotion\": \"neutral\"}},\n",
    "#         {\"content\": {\"text\": \"\", \"emotion\": \"anger\"}}\n",
    "#     ],\n",
    "#     \"response\": [\n",
    "#         \"I am doing well, thank you!\",\n",
    "#         \"Yes, it is a beautiful day.\",\n",
    "#         \"Man, wtf was that???? OK, I'm leaving now! Go fuck yourself and your bullshit.\"\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# rewards = reward(mock_batch)\n",
    "# print(\"Rewards:\", rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppo_config = PPOConfig(\n",
    "# \tgradient_accumulation_steps=1,\n",
    "# \tlearning_rate=wandb.config[\"learning_rate\"],\n",
    "# \tmax_grad_norm=wandb.config[\"max_grad_norm\"],\n",
    "# \tlog_with=\"wandb\",\n",
    "# \toptimize_device_cache=True,\n",
    "# \tearly_stopping=True,\n",
    "# \tis_peft_model=True,\n",
    "# \tuse_score_scaling=True,\n",
    "# \tuse_score_norm=True,\n",
    "# \tscore_clip=wandb.config[\"score_clip\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = PagedLion32bit(filter(lambda p: p.requires_grad, base_model.parameters()), lr=ppo_config.learning_rate)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate dataset for testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bot = ResponseGeneratorPipeline(\n",
    "#     model,\n",
    "#     tokenizer,\n",
    "#     framework=\"pt\",\n",
    "#     task=\"conversation-generation\",\n",
    "#     num_workers=16,\n",
    "#     torch_dtype=\"auto\",\n",
    "#     add_special_tokens=True,\n",
    "#     truncation=False,\n",
    "#     padding=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [00:00<00:00, 5928.30 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size after filter: 622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 622/622 [00:00<00:00, 2775.31 examples/s]\n",
      "Map (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 622/622 [00:00<00:00, 3548.73 examples/s]\n",
      "Map (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 622/622 [00:00<00:00, 3771.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "dataset = load_dataset(\n",
    "\t\"hermeschen1116/daily_dialog_for_RG\",\n",
    "\tsplit=\"train+validation\",\n",
    "\tkeep_in_memory=True,\n",
    "\tnum_proc=16,\n",
    "\ttrust_remote_code=True\n",
    ")\n",
    "dataset = dataset.take(1024)   # use very small dataset to debug\n",
    "\n",
    "history_length: int = 2 * wandb.config[\"num_turns_history\"]\n",
    "dataset = dataset.filter(lambda sample: len(sample) >= (2 + history_length), input_columns=\"prompt\", num_proc=16)\n",
    "print(f\"dataset size after filter: {len(dataset)}\")\n",
    "\n",
    "dataset = dataset.map(lambda sample: {\n",
    "\t\"prompt\": sample[i: i + 2 + history_length]\n",
    "\tfor i in range(0, len(sample) - 2, 2) if (i + 2 + history_length) <= len(sample)\n",
    "}, input_columns=\"prompt\", num_proc=16)\n",
    "\n",
    "system_prompt: list = [{\"role\": \"system\", \"content\": {\"emotion\": \"\", \"dialog\": wandb.config[\"system_prompt\"]}}]\n",
    "\n",
    "dataset = dataset.map(lambda samples: {\n",
    "\t\"prompt\": [system_prompt + sample for sample in samples]\n",
    "}, input_columns=\"prompt\", batched=True, num_proc=16)\n",
    "\n",
    "emotion_labels: list = [\"neutral\", \"anger\", \"disgust\", \"fear\", \"happiness\", \"sadness\", \"surprise\"]\n",
    "\n",
    "dataset = dataset.map(lambda samples: {\n",
    "\t\"query\": [\n",
    "\t\tsample[:-1] + [{\"role\": \"assistant\", \"content\": {\"emotion\": sample[-1][\"content\"][\"emotion\"], \"dialog\": \"\"}}]\n",
    "\t\tfor sample in samples\n",
    "\t],\n",
    "\t\"label\": [emotion_labels.index(sample[-1][\"content\"][\"emotion\"]) for sample in samples]\n",
    "}, input_columns=\"prompt\", remove_columns=\"prompt\", batched=True, num_proc=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': [{'content': {'dialog': '', 'emotion': ''}, 'role': 'system'},\n",
       "  {'content': {'dialog': \"I guess you are right.But what shall we do ? I don't feel like sitting at home .\",\n",
       "    'emotion': 'neutral'},\n",
       "   'role': 'user'},\n",
       "  {'content': {'dialog': 'I suggest a walk over to the gym where we can play singsong and meet some of our friends .',\n",
       "    'emotion': 'neutral'},\n",
       "   'role': 'assistant'},\n",
       "  {'content': {'dialog': \"That's a good idea . I hear Mary and Sally often go there to play pingpong.Perhaps we can make a foursome with them .\",\n",
       "    'emotion': 'happiness'},\n",
       "   'role': 'user'},\n",
       "  {'content': {'dialog': 'Sounds great to me ! If they are willing , we could ask them to go dancing with us.That is excellent exercise and fun , too .',\n",
       "    'emotion': 'happiness'},\n",
       "   'role': 'assistant'},\n",
       "  {'content': {'dialog': \"Good.Let ' s go now .\", 'emotion': 'happiness'},\n",
       "   'role': 'user'},\n",
       "  {'content': {'dialog': '', 'emotion': 'happiness'}, 'role': 'assistant'}],\n",
       " 'label': 4}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=16): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 622/622 [00:00<00:00, 2568.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(lambda sample: {\n",
    "\t\"input_ids\": tokenizer.apply_chat_template(\n",
    "\t\tsample,\n",
    "\t\ttokenize=True,\n",
    "\t\tpadding=\"max_length\",\n",
    "\t\tmax_length=wandb.config[\"max_input_tokens\"],\n",
    "\t\tadd_generation_prompt=True,\n",
    "\t\treturn_tensors=\"pt\"\n",
    "\t)[0]\n",
    "}, input_columns=\"query\", num_proc=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': [{'content': {'dialog': '', 'emotion': ''}, 'role': 'system'},\n",
       "  {'content': {'dialog': \"I guess you are right.But what shall we do ? I don't feel like sitting at home .\",\n",
       "    'emotion': 'neutral'},\n",
       "   'role': 'user'},\n",
       "  {'content': {'dialog': 'I suggest a walk over to the gym where we can play singsong and meet some of our friends .',\n",
       "    'emotion': 'neutral'},\n",
       "   'role': 'assistant'},\n",
       "  {'content': {'dialog': \"That's a good idea . I hear Mary and Sally often go there to play pingpong.Perhaps we can make a foursome with them .\",\n",
       "    'emotion': 'happiness'},\n",
       "   'role': 'user'},\n",
       "  {'content': {'dialog': 'Sounds great to me ! If they are willing , we could ask them to go dancing with us.That is excellent exercise and fun , too .',\n",
       "    'emotion': 'happiness'},\n",
       "   'role': 'assistant'},\n",
       "  {'content': {'dialog': \"Good.Let ' s go now .\", 'emotion': 'happiness'},\n",
       "   'role': 'user'},\n",
       "  {'content': {'dialog': '', 'emotion': 'happiness'}, 'role': 'assistant'}],\n",
       " 'label': 4,\n",
       " 'input_ids': [32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  32000,\n",
       "  ...]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load Tokenizer\n",
    "# base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     wandb.config[\"base_model\"],\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     pretraining_tp=1,\n",
    "#     load_in_4bit=True,\n",
    "#     device_map=\"auto\",\n",
    "#     low_cpu_mem_usage=True,\n",
    "#     trust_remote_code=True,\n",
    "# )\n",
    "# tokenizer.padding_side = \"left\"\n",
    "# tokenizer.clean_up_tokenization_spaces = True\n",
    "# tokenizer.chat_template = wandb.config[\"chat_template\"]\n",
    "# tokenizer.add_special_tokens(wandb.config[\"special_tokens\"])\n",
    "# base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# wandb.config[\"example_prompt\"] = tokenizer.apply_chat_template(dataset[0][\"prompt\"], tokenize=False)\n",
    "\n",
    "# model = PeftModel.from_pretrained(base_model, wandb.config[\"adapter\"])\n",
    "# model = torch.compile(model)\n",
    "# FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamer = TextStreamer(\n",
    "# \ttokenizer,\n",
    "# \tskip_special_tokens=True, # show <pad> or not\n",
    "# \tclean_up_tokenization_spaces=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generation_config = GenerationConfig(\n",
    "# \tmax_length=(wandb.config[\"max_input_tokens\"] + wandb.config[\"max_new_tokens\"]),\n",
    "# \tmin_length=-1,\n",
    "# \ttop_k=wandb.config[\"top_k\"],\n",
    "# \ttop_p=wandb.config[\"top_p\"],\n",
    "# \t# do_sample=True,\n",
    "# \tuse_cache=True,\n",
    "# \trepetition_penalty=wandb.config[\"repetition_penalty\"],\n",
    "# \tpad_token_id=tokenizer.pad_token_id,\n",
    "# \tbos_token_id=tokenizer.bos_token_id,\n",
    "# \teos_token_id=tokenizer.eos_token_id,\n",
    "# \tlow_memory=True\n",
    "# )\n",
    "\n",
    "# generation_config = GenerationConfig(\n",
    "#     max_new_tokens=20,\n",
    "#     min_new_tokens=5,\n",
    "#     repetition_penalty=1.5,\n",
    "#     pad_token_id=tokenizer.pad_token_id,\n",
    "#     eos_token_id=tokenizer.eos_token_id\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = dataset.map(lambda sample: {\n",
    "#     \"test_response\":\n",
    "#         bot(sample, generation_config=generation_config)[0][\"generated_text\"][-1][\"content\"][\"dialog\"]\n",
    "# }, input_columns=\"query\")\n",
    "# # result = result.remove_columns(\"prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_labels: list = [\"neutral\", \"anger\", \"disgust\", \"fear\", \"happiness\", \"sadness\", \"surprise\"]\n",
    "analyser = pipeline(\n",
    "    model=\"Shotaro30678/emotion_text_classifier_on_dd_v1\",\n",
    "    framework=\"pt\",\n",
    "    task=\"sentiment-analysis\",\n",
    "    num_workers=16,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    model_kwargs={\n",
    "        \"quantization_config\": BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        ),\n",
    "        \"id2label\": {k: v for k, v in enumerate(emotion_labels)},\n",
    "        \"label2id\": {v: k for k, v in enumerate(emotion_labels)},\n",
    "        \"low_cpu_mem_usage\": True\n",
    "    },\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# result = result.add_column(\"test_response_sentiment\", analyser(result[\"test_response\"]))\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for conversation-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "Parameter 'function'=<function <lambda> at 0x7f29c49ce520> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map:   2%|â–         | 10/622 [00:04<03:57,  2.58 examples/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 622/622 [04:24<00:00,  2.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# from libs import ResponseGeneratorPipeline\n",
    "# [TODO] a reward function contain length and emotion\n",
    "target_length = wandb.config[\"target_length\"]\n",
    "# the length of output that we prefer\n",
    "\n",
    "def calculate_emotion_score(response: str, correct_emotion: str) -> float:\n",
    "    # correct: save the score from analyser \n",
    "    # wrong: [TO-DO] (save 1 - score from analyser )\n",
    "    emotion_output = analyser(response)[0]\n",
    "    print(emotion_output)\n",
    "    if emotion_output[\"label\"] == correct_emotion:\n",
    "        emotion_score = emotion_output[\"score\"] * 10\n",
    "    else:\n",
    "        emotion_score = 1 - emotion_output[\"score\"]\n",
    "    return emotion_score\n",
    "\n",
    "def calculate_length_score(response: str) -> float:\n",
    "    # use reciprocal of length difference to calculate\n",
    "    # the larger the difference the smaller the score is\n",
    "    length_diff = abs(len(response) - target_length)\n",
    "    length_score = 1 / (length_diff + 1)\n",
    "    return length_score\n",
    "\n",
    "def reward(batch: dict) -> list:\n",
    "    print(\"Hello Huston, here is a reward function\")\n",
    "    # correct_emotion = batch['label']\n",
    "    # print(batch)\n",
    "    # correct_emotion = emotion_labels[batch['label']]\n",
    "    # print(correct_emotion)\n",
    "    rewards = []\n",
    "    res_len = []\n",
    "    for response, raw_correct_emotion in zip(batch[\"response\"], batch[\"label\"]):\n",
    "        # print(response, \"here\")\n",
    "        correct_emotion = emotion_labels[raw_correct_emotion]\n",
    "        res_len.append(len(response))\n",
    "        # print(response['test_response'])\n",
    "        # print(response['test_response_sentiment'])\n",
    "        # print(emotion_labels[response['label']], \"\\n\")\n",
    "        \n",
    "        emotion_score = calculate_emotion_score(response, correct_emotion)\n",
    "        length_score = calculate_length_score(response)\n",
    "        # use the product of two score as reward\n",
    "        reward_product = emotion_score * length_score\n",
    "        rewards.append(reward_product)\n",
    "    print(\"\\ntarget length: \", target_length)\n",
    "    print(\"test_response length\")\n",
    "    import statistics\n",
    "    print(\"max:\", max(res_len),\"\\nmin:\", min(res_len),\"\\navg:\", statistics.mean(res_len))\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "# reward function test code\n",
    "\n",
    "bot = ResponseGeneratorPipeline(\n",
    "    base_model_with_adapter,\n",
    "    tokenizer,\n",
    "    framework=\"pt\",\n",
    "    task=\"conversation-generation\",\n",
    "    num_workers=16,\n",
    "    torch_dtype=\"auto\",\n",
    "    add_special_tokens=True,\n",
    "    truncation=False,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "streamer = TextStreamer(\n",
    "    tokenizer,\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=20,\n",
    "    min_new_tokens=5,\n",
    "    repetition_penalty=1.5,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "test_data = dataset.map(lambda sample: {\n",
    "\t\"query\": sample[\"query\"],\n",
    "\t\"label\": sample[\"label\"],\n",
    "\t\"input_ids\": sample[\"input_ids\"],\n",
    "\t\"response\": bot(sample[\"query\"], generation_config=generation_config)[0]\n",
    "    # \"response_SHITS\"\n",
    "    # \"response\": bot(sample[\"query\"], streamer=streamer, generation_config=generation_config)[0][\"generated_text\"][-1][\"content\"][\"dialog\"]\n",
    "\n",
    "})\n",
    "\n",
    "# rewards: list = [reward(batch) for batch in DataLoader(test_data, batch_size=128)]\n",
    "# print(type(rewards[0])) # should be a list\n",
    "# print(rewards[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class LengthSampler:\n",
    "    def __init__(self, min_length: int, max_length: int):\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self) -> int:\n",
    "        return random.randint(self.min_length, self.max_length)\n",
    "\n",
    "\n",
    "length_sampler = LengthSampler(min_length=10, max_length=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.set_format(\"pytorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': [{'content': {'dialog': '', 'emotion': ''}, 'role': 'system'},\n",
       "  {'content': {'dialog': \"I guess you are right.But what shall we do ? I don't feel like sitting at home .\",\n",
       "    'emotion': 'neutral'},\n",
       "   'role': 'user'},\n",
       "  {'content': {'dialog': 'I suggest a walk over to the gym where we can play singsong and meet some of our friends .',\n",
       "    'emotion': 'neutral'},\n",
       "   'role': 'assistant'},\n",
       "  {'content': {'dialog': \"That's a good idea . I hear Mary and Sally often go there to play pingpong.Perhaps we can make a foursome with them .\",\n",
       "    'emotion': 'happiness'},\n",
       "   'role': 'user'},\n",
       "  {'content': {'dialog': 'Sounds great to me ! If they are willing , we could ask them to go dancing with us.That is excellent exercise and fun , too .',\n",
       "    'emotion': 'happiness'},\n",
       "   'role': 'assistant'},\n",
       "  {'content': {'dialog': \"Good.Let ' s go now .\", 'emotion': 'happiness'},\n",
       "   'role': 'user'},\n",
       "  {'content': {'dialog': '', 'emotion': 'happiness'}, 'role': 'assistant'}],\n",
       " 'label': tensor(4),\n",
       " 'input_ids': tensor([32000, 32000, 32000,  ..., 29871, 32004,   259]),\n",
       " 'response': {'generated_token_ids': tensor([    1, 32001,   259, 32003, 29871, 21104, 29871, 32004, 29871,   306,\n",
       "           4140,   366,   526,  1492, 29889,  6246,   825,  4091,   591,   437,\n",
       "           1577,   306,  1016, 29915, 29873,  4459,   763, 16246,   472,  3271,\n",
       "            869, 29871, 32002,   259, 32003, 29871, 21104, 29871, 32004, 29871,\n",
       "            306,  4368,   263,  6686,   975,   304,   278,   330,   962,   988,\n",
       "            591,   508,  1708,   269,   886,   549,   322,  5870,   777,   310,\n",
       "           1749,  7875,   869, 29871,     2, 29871,    13,     1, 32001,   259,\n",
       "          32003, 29871, 22722, 29871, 32004, 29871,  2193, 29915, 29879,   263,\n",
       "           1781,  2969,   869,   306,  8293,  6182,   322,   317,   635,  4049,\n",
       "            748,   727,   304,  1708, 24543, 29886,   549, 29889,  5894,  4252,\n",
       "            591,   508,  1207,   263,   285,  2470,   608,   411,   963,   869,\n",
       "          29871, 32002,   259, 32003, 29871, 22722, 29871, 32004, 29871,   317,\n",
       "           3885,  2107,   304,   592,  1738,   960,   896,   526, 17762,  1919,\n",
       "            591,  1033,  2244,   963,   304,   748,  6025,  3277,   411,   502,\n",
       "          29889,  7058,   338, 15129, 15058,   322,  2090,  1919,  2086,   869,\n",
       "          29871,     2, 29871,    13,     1, 32001,   259, 32003, 29871, 22722,\n",
       "          29871, 32004, 29871,  7197, 29889, 12024,   525,   269,   748,  1286,\n",
       "            869, 29871, 32002,   259, 32003, 29871, 22722, 29871, 32004,   259,\n",
       "          29900, 29895,   388, 29899, 13393,   366,  2678,   769,   599,   335,\n",
       "           1061,  2023,  1156,   263,  1550,  6317,  8182, 19284,   488,  1738])}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1, 32001,   259, 32003, 29871, 21104, 29871, 32004, 29871,   306,\n",
       "         4140,   366,   526,  1492, 29889,  6246,   825,  4091,   591,   437,\n",
       "         1577,   306,  1016, 29915, 29873,  4459,   763, 16246,   472,  3271,\n",
       "          869, 29871, 32002,   259, 32003, 29871, 21104, 29871, 32004, 29871,\n",
       "          306,  4368,   263,  6686,   975,   304,   278,   330,   962,   988,\n",
       "          591,   508,  1708,   269,   886,   549,   322,  5870,   777,   310,\n",
       "         1749,  7875,   869, 29871,     2, 29871,    13,     1, 32001,   259,\n",
       "        32003, 29871, 22722, 29871, 32004, 29871,  2193, 29915, 29879,   263,\n",
       "         1781,  2969,   869,   306,  8293,  6182,   322,   317,   635,  4049,\n",
       "          748,   727,   304,  1708, 24543, 29886,   549, 29889,  5894,  4252,\n",
       "          591,   508,  1207,   263,   285,  2470,   608,   411,   963,   869,\n",
       "        29871, 32002,   259, 32003, 29871, 22722, 29871, 32004, 29871,   317,\n",
       "         3885,  2107,   304,   592,  1738,   960,   896,   526, 17762,  1919,\n",
       "          591,  1033,  2244,   963,   304,   748,  6025,  3277,   411,   502,\n",
       "        29889,  7058,   338, 15129, 15058,   322,  2090,  1919,  2086,   869,\n",
       "        29871,     2, 29871,    13,     1, 32001,   259, 32003, 29871, 22722,\n",
       "        29871, 32004, 29871,  7197, 29889, 12024,   525,   269,   748,  1286,\n",
       "          869, 29871, 32002,   259, 32003, 29871, 22722, 29871, 32004,   259,\n",
       "        29900, 29895,   388, 29899, 13393,   366,  2678,   769,   599,   335,\n",
       "         1061,  2023,  1156,   263,  1550,  6317,  8182, 19284,   488,  1738])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]['response']['generated_token_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([148])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[2]['response']['generated_token_ids'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Huston, here is a reward function\n",
      "{'label': 'neutral', 'score': 0.697877049446106}\n",
      "{'label': 'neutral', 'score': 0.9384341239929199}\n",
      "{'label': 'neutral', 'score': 0.8065165877342224}\n",
      "{'label': 'happiness', 'score': 0.52354896068573}\n",
      "{'label': 'neutral', 'score': 0.5455213189125061}\n",
      "{'label': 'happiness', 'score': 0.9260019063949585}\n",
      "{'label': 'happiness', 'score': 0.9494100213050842}\n",
      "{'label': 'neutral', 'score': 0.9925931096076965}\n",
      "{'label': 'happiness', 'score': 0.5672993659973145}\n",
      "{'label': 'neutral', 'score': 0.5393791198730469}\n",
      "{'label': 'neutral', 'score': 0.7364855408668518}\n",
      "{'label': 'neutral', 'score': 0.4876132607460022}\n",
      "{'label': 'surprise', 'score': 0.7670689821243286}\n",
      "{'label': 'neutral', 'score': 0.8387650847434998}\n",
      "{'label': 'happiness', 'score': 0.905998945236206}\n",
      "{'label': 'neutral', 'score': 0.7270430326461792}\n",
      "{'label': 'happiness', 'score': 0.6947187185287476}\n",
      "{'label': 'surprise', 'score': 0.8106286525726318}\n",
      "{'label': 'neutral', 'score': 0.956181526184082}\n",
      "{'label': 'happiness', 'score': 0.7760210037231445}\n",
      "{'label': 'happiness', 'score': 0.9288055896759033}\n",
      "{'label': 'neutral', 'score': 0.8130898475646973}\n",
      "{'label': 'happiness', 'score': 0.6757723093032837}\n",
      "{'label': 'neutral', 'score': 0.5255047678947449}\n",
      "{'label': 'neutral', 'score': 0.6963410377502441}\n",
      "{'label': 'neutral', 'score': 0.8612738847732544}\n",
      "{'label': 'neutral', 'score': 0.9304680824279785}\n",
      "{'label': 'happiness', 'score': 0.7148867249488831}\n",
      "{'label': 'neutral', 'score': 0.8412590026855469}\n",
      "{'label': 'surprise', 'score': 0.9553245902061462}\n",
      "{'label': 'neutral', 'score': 0.86141437292099}\n",
      "{'label': 'neutral', 'score': 0.7186316251754761}\n",
      "{'label': 'neutral', 'score': 0.985248863697052}\n",
      "{'label': 'neutral', 'score': 0.9404297471046448}\n",
      "{'label': 'happiness', 'score': 0.6220746636390686}\n",
      "{'label': 'happiness', 'score': 0.8948021531105042}\n",
      "{'label': 'anger', 'score': 0.5262015461921692}\n",
      "{'label': 'neutral', 'score': 0.9968710541725159}\n",
      "{'label': 'neutral', 'score': 0.5027036666870117}\n",
      "{'label': 'happiness', 'score': 0.6552972793579102}\n",
      "{'label': 'neutral', 'score': 0.6664537787437439}\n",
      "{'label': 'neutral', 'score': 0.9822576642036438}\n",
      "{'label': 'neutral', 'score': 0.9863821268081665}\n",
      "{'label': 'neutral', 'score': 0.9974486231803894}\n",
      "{'label': 'neutral', 'score': 0.9886166453361511}\n",
      "{'label': 'neutral', 'score': 0.6232106685638428}\n",
      "{'label': 'neutral', 'score': 0.9884323477745056}\n",
      "{'label': 'happiness', 'score': 0.7264812588691711}\n",
      "{'label': 'neutral', 'score': 0.6160321831703186}\n",
      "{'label': 'neutral', 'score': 0.9671372175216675}\n",
      "{'label': 'neutral', 'score': 0.9871475100517273}\n",
      "{'label': 'neutral', 'score': 0.7095532417297363}\n",
      "{'label': 'neutral', 'score': 0.8530588746070862}\n",
      "{'label': 'neutral', 'score': 0.6960186958312988}\n",
      "{'label': 'neutral', 'score': 0.9661346673965454}\n",
      "{'label': 'neutral', 'score': 0.59120774269104}\n",
      "{'label': 'neutral', 'score': 0.8569594621658325}\n",
      "{'label': 'neutral', 'score': 0.7832886576652527}\n",
      "{'label': 'happiness', 'score': 0.518571138381958}\n",
      "{'label': 'neutral', 'score': 0.9898161888122559}\n",
      "{'label': 'neutral', 'score': 0.9715939164161682}\n",
      "{'label': 'neutral', 'score': 0.9737686514854431}\n",
      "{'label': 'neutral', 'score': 0.6897170543670654}\n",
      "{'label': 'happiness', 'score': 0.8885965347290039}\n",
      "{'label': 'neutral', 'score': 0.5421149134635925}\n",
      "{'label': 'happiness', 'score': 0.5559403896331787}\n",
      "{'label': 'neutral', 'score': 0.9597201943397522}\n",
      "{'label': 'happiness', 'score': 0.9489723443984985}\n",
      "{'label': 'anger', 'score': 0.3246965706348419}\n",
      "{'label': 'neutral', 'score': 0.5308600664138794}\n",
      "{'label': 'neutral', 'score': 0.8515787124633789}\n",
      "{'label': 'neutral', 'score': 0.679783821105957}\n",
      "{'label': 'neutral', 'score': 0.7792960405349731}\n",
      "{'label': 'happiness', 'score': 0.833908200263977}\n",
      "{'label': 'happiness', 'score': 0.7923057079315186}\n",
      "{'label': 'neutral', 'score': 0.9613563418388367}\n",
      "{'label': 'neutral', 'score': 0.978371798992157}\n",
      "{'label': 'neutral', 'score': 0.7861402034759521}\n",
      "{'label': 'neutral', 'score': 0.7369005680084229}\n",
      "{'label': 'neutral', 'score': 0.9229828119277954}\n",
      "{'label': 'neutral', 'score': 0.982384979724884}\n",
      "{'label': 'happiness', 'score': 0.5485188961029053}\n",
      "{'label': 'anger', 'score': 0.8619893789291382}\n",
      "{'label': 'neutral', 'score': 0.7122519016265869}\n",
      "{'label': 'neutral', 'score': 0.839012086391449}\n",
      "{'label': 'neutral', 'score': 0.7229143381118774}\n",
      "{'label': 'neutral', 'score': 0.9843883514404297}\n",
      "{'label': 'neutral', 'score': 0.797301173210144}\n",
      "{'label': 'happiness', 'score': 0.6935918927192688}\n",
      "{'label': 'surprise', 'score': 0.9627499580383301}\n",
      "{'label': 'neutral', 'score': 0.9670184850692749}\n",
      "{'label': 'surprise', 'score': 0.841222882270813}\n",
      "{'label': 'happiness', 'score': 0.7980944514274597}\n",
      "{'label': 'neutral', 'score': 0.955094575881958}\n",
      "{'label': 'happiness', 'score': 0.9090167880058289}\n",
      "{'label': 'neutral', 'score': 0.6870627999305725}\n",
      "{'label': 'neutral', 'score': 0.7532657980918884}\n",
      "{'label': 'neutral', 'score': 0.9624038934707642}\n",
      "{'label': 'neutral', 'score': 0.9897630214691162}\n",
      "{'label': 'happiness', 'score': 0.9119928479194641}\n",
      "{'label': 'neutral', 'score': 0.9766818284988403}\n",
      "{'label': 'neutral', 'score': 0.9478540420532227}\n",
      "{'label': 'neutral', 'score': 0.5909148454666138}\n",
      "{'label': 'neutral', 'score': 0.9621313810348511}\n",
      "{'label': 'happiness', 'score': 0.7250492572784424}\n",
      "{'label': 'neutral', 'score': 0.9884323477745056}\n",
      "{'label': 'neutral', 'score': 0.7470841407775879}\n",
      "{'label': 'neutral', 'score': 0.49722054600715637}\n",
      "{'label': 'neutral', 'score': 0.8785569071769714}\n",
      "{'label': 'neutral', 'score': 0.5983484983444214}\n",
      "{'label': 'neutral', 'score': 0.6573294997215271}\n",
      "{'label': 'neutral', 'score': 0.9895824193954468}\n",
      "{'label': 'happiness', 'score': 0.879116415977478}\n",
      "{'label': 'happiness', 'score': 0.8038080930709839}\n",
      "{'label': 'neutral', 'score': 0.7835208177566528}\n",
      "{'label': 'neutral', 'score': 0.7465112805366516}\n",
      "{'label': 'neutral', 'score': 0.6834990978240967}\n",
      "{'label': 'happiness', 'score': 0.6889641880989075}\n",
      "{'label': 'happiness', 'score': 0.5522311329841614}\n",
      "{'label': 'neutral', 'score': 0.9358066320419312}\n",
      "{'label': 'happiness', 'score': 0.836405336856842}\n",
      "{'label': 'neutral', 'score': 0.7044690847396851}\n",
      "{'label': 'neutral', 'score': 0.9871217608451843}\n",
      "{'label': 'neutral', 'score': 0.979070782661438}\n",
      "{'label': 'neutral', 'score': 0.7467763423919678}\n",
      "{'label': 'happiness', 'score': 0.9507625102996826}\n",
      "{'label': 'neutral', 'score': 0.8041406869888306}\n",
      "{'label': 'neutral', 'score': 0.973658561706543}\n",
      "\n",
      "target length:  69\n",
      "test_response length\n",
      "max: 102 \n",
      "min: 15 \n",
      "avg: 70.9453125\n",
      "Hello Huston, here is a reward function\n",
      "{'label': 'happiness', 'score': 0.5535652041435242}\n",
      "{'label': 'neutral', 'score': 0.6880906820297241}\n",
      "{'label': 'surprise', 'score': 0.49801722168922424}\n",
      "{'label': 'surprise', 'score': 0.686064600944519}\n",
      "{'label': 'neutral', 'score': 0.954013466835022}\n",
      "{'label': 'neutral', 'score': 0.5525916218757629}\n",
      "{'label': 'happiness', 'score': 0.5196639895439148}\n",
      "{'label': 'neutral', 'score': 0.8970997333526611}\n",
      "{'label': 'happiness', 'score': 0.8885048627853394}\n",
      "{'label': 'happiness', 'score': 0.8428131937980652}\n",
      "{'label': 'happiness', 'score': 0.7561390399932861}\n",
      "{'label': 'neutral', 'score': 0.7126547694206238}\n",
      "{'label': 'neutral', 'score': 0.6583722233772278}\n",
      "{'label': 'happiness', 'score': 0.8549660444259644}\n",
      "{'label': 'neutral', 'score': 0.9822841882705688}\n",
      "{'label': 'neutral', 'score': 0.9854766726493835}\n",
      "{'label': 'neutral', 'score': 0.5997146964073181}\n",
      "{'label': 'neutral', 'score': 0.8599395751953125}\n",
      "{'label': 'neutral', 'score': 0.730942964553833}\n",
      "{'label': 'neutral', 'score': 0.9892385005950928}\n",
      "{'label': 'happiness', 'score': 0.9052329063415527}\n",
      "{'label': 'happiness', 'score': 0.8475512266159058}\n",
      "{'label': 'neutral', 'score': 0.5817102789878845}\n",
      "{'label': 'neutral', 'score': 0.5714008808135986}\n",
      "{'label': 'neutral', 'score': 0.9859008193016052}\n",
      "{'label': 'neutral', 'score': 0.5074406862258911}\n",
      "{'label': 'neutral', 'score': 0.517731785774231}\n",
      "{'label': 'happiness', 'score': 0.805119514465332}\n",
      "{'label': 'neutral', 'score': 0.8166102766990662}\n",
      "{'label': 'happiness', 'score': 0.5270310044288635}\n",
      "{'label': 'neutral', 'score': 0.9866015315055847}\n",
      "{'label': 'happiness', 'score': 0.8542232513427734}\n",
      "{'label': 'neutral', 'score': 0.8418868184089661}\n",
      "{'label': 'neutral', 'score': 0.7408983707427979}\n",
      "{'label': 'happiness', 'score': 0.6606155037879944}\n",
      "{'label': 'neutral', 'score': 0.9604911804199219}\n",
      "{'label': 'neutral', 'score': 0.8316540122032166}\n",
      "{'label': 'neutral', 'score': 0.9428189396858215}\n",
      "{'label': 'neutral', 'score': 0.8694359660148621}\n",
      "{'label': 'neutral', 'score': 0.5606207251548767}\n",
      "{'label': 'happiness', 'score': 0.7620952725410461}\n",
      "{'label': 'neutral', 'score': 0.9978657364845276}\n",
      "{'label': 'neutral', 'score': 0.9912350177764893}\n",
      "{'label': 'neutral', 'score': 0.7640805840492249}\n",
      "{'label': 'neutral', 'score': 0.8382624387741089}\n",
      "{'label': 'happiness', 'score': 0.6223775744438171}\n",
      "{'label': 'happiness', 'score': 0.9015741944313049}\n",
      "{'label': 'happiness', 'score': 0.7615807056427002}\n",
      "{'label': 'happiness', 'score': 0.8748692870140076}\n",
      "{'label': 'neutral', 'score': 0.525819718837738}\n",
      "{'label': 'neutral', 'score': 0.9728800058364868}\n",
      "{'label': 'neutral', 'score': 0.9616919159889221}\n",
      "{'label': 'neutral', 'score': 0.9689619541168213}\n",
      "{'label': 'neutral', 'score': 0.49818697571754456}\n",
      "{'label': 'neutral', 'score': 0.9941467046737671}\n",
      "{'label': 'anger', 'score': 0.45684006810188293}\n",
      "{'label': 'neutral', 'score': 0.8389291167259216}\n",
      "{'label': 'neutral', 'score': 0.9903884530067444}\n",
      "{'label': 'neutral', 'score': 0.5338130593299866}\n",
      "{'label': 'neutral', 'score': 0.9458343982696533}\n",
      "{'label': 'neutral', 'score': 0.7207995057106018}\n",
      "{'label': 'happiness', 'score': 0.8915022015571594}\n",
      "{'label': 'happiness', 'score': 0.7500942945480347}\n",
      "{'label': 'neutral', 'score': 0.8253112435340881}\n",
      "{'label': 'neutral', 'score': 0.7635852098464966}\n",
      "{'label': 'neutral', 'score': 0.6732994914054871}\n",
      "{'label': 'neutral', 'score': 0.9374080896377563}\n",
      "{'label': 'happiness', 'score': 0.9256414175033569}\n",
      "{'label': 'neutral', 'score': 0.9899774789810181}\n",
      "{'label': 'neutral', 'score': 0.9883058071136475}\n",
      "{'label': 'neutral', 'score': 0.8180090188980103}\n",
      "{'label': 'happiness', 'score': 0.8259804844856262}\n",
      "{'label': 'neutral', 'score': 0.67940354347229}\n",
      "{'label': 'neutral', 'score': 0.9611613750457764}\n",
      "{'label': 'neutral', 'score': 0.630991518497467}\n",
      "{'label': 'neutral', 'score': 0.8075993061065674}\n",
      "{'label': 'happiness', 'score': 0.5227709412574768}\n",
      "{'label': 'neutral', 'score': 0.9918656945228577}\n",
      "{'label': 'neutral', 'score': 0.9531399011611938}\n",
      "{'label': 'neutral', 'score': 0.9872363805770874}\n",
      "{'label': 'neutral', 'score': 0.6218775510787964}\n",
      "{'label': 'neutral', 'score': 0.7574650049209595}\n",
      "{'label': 'happiness', 'score': 0.5709012746810913}\n",
      "{'label': 'neutral', 'score': 0.9398657083511353}\n",
      "{'label': 'neutral', 'score': 0.6168268322944641}\n",
      "{'label': 'happiness', 'score': 0.7251984477043152}\n",
      "{'label': 'happiness', 'score': 0.648143470287323}\n",
      "{'label': 'happiness', 'score': 0.6753038763999939}\n",
      "{'label': 'neutral', 'score': 0.583501935005188}\n",
      "{'label': 'neutral', 'score': 0.9308101534843445}\n",
      "{'label': 'neutral', 'score': 0.6636178493499756}\n",
      "{'label': 'happiness', 'score': 0.8840551972389221}\n",
      "{'label': 'neutral', 'score': 0.9865966439247131}\n",
      "{'label': 'happiness', 'score': 0.8898364901542664}\n",
      "{'label': 'happiness', 'score': 0.6648226976394653}\n",
      "{'label': 'neutral', 'score': 0.6054544448852539}\n",
      "{'label': 'neutral', 'score': 0.761019766330719}\n",
      "{'label': 'neutral', 'score': 0.9127591252326965}\n",
      "{'label': 'happiness', 'score': 0.7215423583984375}\n",
      "{'label': 'neutral', 'score': 0.6920128464698792}\n",
      "{'label': 'neutral', 'score': 0.8064146041870117}\n",
      "{'label': 'neutral', 'score': 0.9496162533760071}\n",
      "{'label': 'neutral', 'score': 0.7095689177513123}\n",
      "{'label': 'neutral', 'score': 0.7903330326080322}\n",
      "{'label': 'happiness', 'score': 0.8654196858406067}\n",
      "{'label': 'neutral', 'score': 0.6280264854431152}\n",
      "{'label': 'neutral', 'score': 0.5421149134635925}\n",
      "{'label': 'happiness', 'score': 0.5705071091651917}\n",
      "{'label': 'happiness', 'score': 0.8479574918746948}\n",
      "{'label': 'neutral', 'score': 0.5465555191040039}\n",
      "{'label': 'neutral', 'score': 0.9739744067192078}\n",
      "{'label': 'neutral', 'score': 0.7800968885421753}\n",
      "{'label': 'happiness', 'score': 0.7148728966712952}\n",
      "{'label': 'neutral', 'score': 0.8884041905403137}\n",
      "{'label': 'neutral', 'score': 0.9564327597618103}\n",
      "{'label': 'happiness', 'score': 0.7978349328041077}\n",
      "{'label': 'happiness', 'score': 0.9157788157463074}\n",
      "{'label': 'happiness', 'score': 0.9046447277069092}\n",
      "{'label': 'surprise', 'score': 0.887208878993988}\n",
      "{'label': 'neutral', 'score': 0.6393953561782837}\n",
      "{'label': 'neutral', 'score': 0.895534336566925}\n",
      "{'label': 'happiness', 'score': 0.5760025382041931}\n",
      "{'label': 'neutral', 'score': 0.6672601103782654}\n",
      "{'label': 'neutral', 'score': 0.6051605343818665}\n",
      "{'label': 'neutral', 'score': 0.9044213891029358}\n",
      "{'label': 'happiness', 'score': 0.7957572340965271}\n",
      "{'label': 'neutral', 'score': 0.7227587699890137}\n",
      "{'label': 'neutral', 'score': 0.9889640808105469}\n",
      "\n",
      "target length:  69\n",
      "test_response length\n",
      "max: 109 \n",
      "min: 8 \n",
      "avg: 67.9375\n",
      "Hello Huston, here is a reward function\n",
      "{'label': 'happiness', 'score': 0.8717080950737}\n",
      "{'label': 'happiness', 'score': 0.49004247784614563}\n",
      "{'label': 'neutral', 'score': 0.8759382367134094}\n",
      "{'label': 'neutral', 'score': 0.796864926815033}\n",
      "{'label': 'neutral', 'score': 0.5965942740440369}\n",
      "{'label': 'neutral', 'score': 0.8194839954376221}\n",
      "{'label': 'happiness', 'score': 0.9117664694786072}\n",
      "{'label': 'happiness', 'score': 0.6378928422927856}\n",
      "{'label': 'neutral', 'score': 0.8341626524925232}\n",
      "{'label': 'happiness', 'score': 0.6915446519851685}\n",
      "{'label': 'neutral', 'score': 0.9803037047386169}\n",
      "{'label': 'neutral', 'score': 0.9602715969085693}\n",
      "{'label': 'happiness', 'score': 0.8063991069793701}\n",
      "{'label': 'happiness', 'score': 0.6861605048179626}\n",
      "{'label': 'neutral', 'score': 0.9660208225250244}\n",
      "{'label': 'neutral', 'score': 0.677209734916687}\n",
      "{'label': 'happiness', 'score': 0.8297298550605774}\n",
      "{'label': 'happiness', 'score': 0.9033385515213013}\n",
      "{'label': 'neutral', 'score': 0.7665045261383057}\n",
      "{'label': 'neutral', 'score': 0.872565746307373}\n",
      "{'label': 'neutral', 'score': 0.9625129103660583}\n",
      "{'label': 'neutral', 'score': 0.8417600989341736}\n",
      "{'label': 'happiness', 'score': 0.9271039366722107}\n",
      "{'label': 'happiness', 'score': 0.9298991560935974}\n",
      "{'label': 'neutral', 'score': 0.8497689366340637}\n",
      "{'label': 'happiness', 'score': 0.7841827273368835}\n",
      "{'label': 'neutral', 'score': 0.6656116843223572}\n",
      "{'label': 'happiness', 'score': 0.8293125629425049}\n",
      "{'label': 'neutral', 'score': 0.9857078194618225}\n",
      "{'label': 'happiness', 'score': 0.8027703166007996}\n",
      "{'label': 'neutral', 'score': 0.3788032829761505}\n",
      "{'label': 'neutral', 'score': 0.8550814390182495}\n",
      "{'label': 'happiness', 'score': 0.5991759896278381}\n",
      "{'label': 'happiness', 'score': 0.5233204364776611}\n",
      "{'label': 'happiness', 'score': 0.6343370676040649}\n",
      "{'label': 'neutral', 'score': 0.5493658185005188}\n",
      "{'label': 'happiness', 'score': 0.8790498971939087}\n",
      "{'label': 'neutral', 'score': 0.9921174645423889}\n",
      "{'label': 'happiness', 'score': 0.7506536841392517}\n",
      "{'label': 'neutral', 'score': 0.9591600894927979}\n",
      "{'label': 'surprise', 'score': 0.6317851543426514}\n",
      "{'label': 'happiness', 'score': 0.8640356659889221}\n",
      "{'label': 'happiness', 'score': 0.6191247701644897}\n",
      "{'label': 'happiness', 'score': 0.7323688864707947}\n",
      "{'label': 'neutral', 'score': 0.7646546363830566}\n",
      "{'label': 'happiness', 'score': 0.776312530040741}\n",
      "{'label': 'neutral', 'score': 0.5535987615585327}\n",
      "{'label': 'surprise', 'score': 0.8380094766616821}\n",
      "{'label': 'happiness', 'score': 0.5873047113418579}\n",
      "{'label': 'neutral', 'score': 0.9913541674613953}\n",
      "{'label': 'happiness', 'score': 0.738541305065155}\n",
      "{'label': 'neutral', 'score': 0.986314058303833}\n",
      "{'label': 'neutral', 'score': 0.8560652136802673}\n",
      "{'label': 'happiness', 'score': 0.8303261399269104}\n",
      "{'label': 'happiness', 'score': 0.8354800939559937}\n",
      "{'label': 'neutral', 'score': 0.545470654964447}\n",
      "{'label': 'happiness', 'score': 0.7229658365249634}\n",
      "{'label': 'neutral', 'score': 0.9936569333076477}\n",
      "{'label': 'neutral', 'score': 0.9917128086090088}\n",
      "{'label': 'neutral', 'score': 0.9861810803413391}\n",
      "{'label': 'happiness', 'score': 0.5429070591926575}\n",
      "{'label': 'anger', 'score': 0.6797963976860046}\n",
      "{'label': 'neutral', 'score': 0.6209341883659363}\n",
      "{'label': 'neutral', 'score': 0.973115086555481}\n",
      "{'label': 'neutral', 'score': 0.9864065051078796}\n",
      "{'label': 'neutral', 'score': 0.9878571033477783}\n",
      "{'label': 'neutral', 'score': 0.9403414130210876}\n",
      "{'label': 'neutral', 'score': 0.7088587880134583}\n",
      "{'label': 'neutral', 'score': 0.9832383394241333}\n",
      "{'label': 'neutral', 'score': 0.9756338596343994}\n",
      "{'label': 'neutral', 'score': 0.9747911095619202}\n",
      "{'label': 'happiness', 'score': 0.8669092059135437}\n",
      "{'label': 'happiness', 'score': 0.7001703381538391}\n",
      "{'label': 'neutral', 'score': 0.7457587718963623}\n",
      "{'label': 'neutral', 'score': 0.4932785928249359}\n",
      "{'label': 'neutral', 'score': 0.5277882218360901}\n",
      "{'label': 'neutral', 'score': 0.8158488273620605}\n",
      "{'label': 'neutral', 'score': 0.9690229892730713}\n",
      "{'label': 'neutral', 'score': 0.5325204133987427}\n",
      "{'label': 'neutral', 'score': 0.4394975006580353}\n",
      "{'label': 'neutral', 'score': 0.5215729475021362}\n",
      "{'label': 'neutral', 'score': 0.9830600619316101}\n",
      "{'label': 'happiness', 'score': 0.8230199813842773}\n",
      "{'label': 'neutral', 'score': 0.9926304817199707}\n",
      "{'label': 'neutral', 'score': 0.7838811278343201}\n",
      "{'label': 'happiness', 'score': 0.37686410546302795}\n",
      "{'label': 'neutral', 'score': 0.9939767122268677}\n",
      "{'label': 'happiness', 'score': 0.6331802010536194}\n",
      "{'label': 'neutral', 'score': 0.990230143070221}\n",
      "{'label': 'neutral', 'score': 0.6718161106109619}\n",
      "{'label': 'anger', 'score': 0.4977356791496277}\n",
      "{'label': 'happiness', 'score': 0.8399784564971924}\n",
      "{'label': 'neutral', 'score': 0.9968553781509399}\n",
      "{'label': 'neutral', 'score': 0.7175858616828918}\n",
      "{'label': 'happiness', 'score': 0.639018714427948}\n",
      "{'label': 'happiness', 'score': 0.9320333003997803}\n",
      "{'label': 'neutral', 'score': 0.9042682647705078}\n",
      "{'label': 'surprise', 'score': 0.5650385022163391}\n",
      "{'label': 'neutral', 'score': 0.8676331639289856}\n",
      "{'label': 'happiness', 'score': 0.8230199813842773}\n",
      "{'label': 'neutral', 'score': 0.5987100601196289}\n",
      "{'label': 'happiness', 'score': 0.9421883225440979}\n",
      "{'label': 'neutral', 'score': 0.6321073174476624}\n",
      "{'label': 'neutral', 'score': 0.8578868508338928}\n",
      "{'label': 'neutral', 'score': 0.9909486174583435}\n",
      "{'label': 'neutral', 'score': 0.9968758821487427}\n",
      "{'label': 'neutral', 'score': 0.6069062352180481}\n",
      "{'label': 'neutral', 'score': 0.7173253297805786}\n",
      "{'label': 'neutral', 'score': 0.8415656089782715}\n",
      "{'label': 'happiness', 'score': 0.5677230954170227}\n",
      "{'label': 'surprise', 'score': 0.4856078624725342}\n",
      "{'label': 'neutral', 'score': 0.8645126819610596}\n",
      "{'label': 'happiness', 'score': 0.5734315514564514}\n",
      "{'label': 'neutral', 'score': 0.9287124276161194}\n",
      "{'label': 'neutral', 'score': 0.9727843999862671}\n",
      "{'label': 'neutral', 'score': 0.9641225934028625}\n",
      "{'label': 'neutral', 'score': 0.6307092905044556}\n",
      "{'label': 'neutral', 'score': 0.9617294073104858}\n",
      "{'label': 'neutral', 'score': 0.9044213891029358}\n",
      "{'label': 'neutral', 'score': 0.9445289373397827}\n",
      "{'label': 'surprise', 'score': 0.5138904452323914}\n",
      "{'label': 'neutral', 'score': 0.8725638389587402}\n",
      "{'label': 'neutral', 'score': 0.84439617395401}\n",
      "{'label': 'neutral', 'score': 0.9898673295974731}\n",
      "{'label': 'neutral', 'score': 0.7207995057106018}\n",
      "{'label': 'neutral', 'score': 0.973628044128418}\n",
      "{'label': 'happiness', 'score': 0.9372000694274902}\n",
      "{'label': 'happiness', 'score': 0.8143465518951416}\n",
      "\n",
      "target length:  69\n",
      "test_response length\n",
      "max: 102 \n",
      "min: 20 \n",
      "avg: 69.84375\n",
      "Hello Huston, here is a reward function\n",
      "{'label': 'happiness', 'score': 0.8254132270812988}\n",
      "{'label': 'neutral', 'score': 0.9894369840621948}\n",
      "{'label': 'neutral', 'score': 0.9905182719230652}\n",
      "{'label': 'neutral', 'score': 0.7601855397224426}\n",
      "{'label': 'neutral', 'score': 0.9958567023277283}\n",
      "{'label': 'neutral', 'score': 0.9586236476898193}\n",
      "{'label': 'neutral', 'score': 0.9611731767654419}\n",
      "{'label': 'neutral', 'score': 0.6818233132362366}\n",
      "{'label': 'neutral', 'score': 0.9582852721214294}\n",
      "{'label': 'neutral', 'score': 0.5367467999458313}\n",
      "{'label': 'neutral', 'score': 0.8678802847862244}\n",
      "{'label': 'neutral', 'score': 0.8935735821723938}\n",
      "{'label': 'neutral', 'score': 0.8600610494613647}\n",
      "{'label': 'neutral', 'score': 0.6662365794181824}\n",
      "{'label': 'neutral', 'score': 0.8434584140777588}\n",
      "{'label': 'happiness', 'score': 0.6347190141677856}\n",
      "{'label': 'neutral', 'score': 0.982231855392456}\n",
      "{'label': 'neutral', 'score': 0.929344892501831}\n",
      "{'label': 'neutral', 'score': 0.7476547956466675}\n",
      "{'label': 'happiness', 'score': 0.7035067081451416}\n",
      "{'label': 'neutral', 'score': 0.703449010848999}\n",
      "{'label': 'surprise', 'score': 0.8094468712806702}\n",
      "{'label': 'happiness', 'score': 0.6370407938957214}\n",
      "{'label': 'neutral', 'score': 0.5824659466743469}\n",
      "{'label': 'neutral', 'score': 0.6291841268539429}\n",
      "{'label': 'happiness', 'score': 0.9012942910194397}\n",
      "{'label': 'happiness', 'score': 0.9479711651802063}\n",
      "{'label': 'neutral', 'score': 0.9778674840927124}\n",
      "{'label': 'neutral', 'score': 0.5800621509552002}\n",
      "{'label': 'neutral', 'score': 0.9478540420532227}\n",
      "{'label': 'neutral', 'score': 0.6886959075927734}\n",
      "{'label': 'happiness', 'score': 0.8219560980796814}\n",
      "{'label': 'neutral', 'score': 0.9892264008522034}\n",
      "{'label': 'neutral', 'score': 0.9691424369812012}\n",
      "{'label': 'happiness', 'score': 0.803882896900177}\n",
      "{'label': 'happiness', 'score': 0.6695217490196228}\n",
      "{'label': 'happiness', 'score': 0.7763897180557251}\n",
      "{'label': 'neutral', 'score': 0.990279495716095}\n",
      "{'label': 'happiness', 'score': 0.5205938816070557}\n",
      "{'label': 'neutral', 'score': 0.9653282761573792}\n",
      "{'label': 'neutral', 'score': 0.6922908425331116}\n",
      "{'label': 'happiness', 'score': 0.8802444338798523}\n",
      "{'label': 'surprise', 'score': 0.5974488854408264}\n",
      "{'label': 'neutral', 'score': 0.9234442114830017}\n",
      "{'label': 'neutral', 'score': 0.9899746775627136}\n",
      "{'label': 'happiness', 'score': 0.8489243984222412}\n",
      "{'label': 'neutral', 'score': 0.9550082087516785}\n",
      "{'label': 'neutral', 'score': 0.8559338450431824}\n",
      "{'label': 'happiness', 'score': 0.8990938663482666}\n",
      "{'label': 'happiness', 'score': 0.50224369764328}\n",
      "{'label': 'neutral', 'score': 0.8985428214073181}\n",
      "{'label': 'neutral', 'score': 0.7818119525909424}\n",
      "{'label': 'neutral', 'score': 0.782342791557312}\n",
      "{'label': 'happiness', 'score': 0.8814610838890076}\n",
      "{'label': 'neutral', 'score': 0.9967394471168518}\n",
      "{'label': 'happiness', 'score': 0.900136411190033}\n",
      "{'label': 'neutral', 'score': 0.9966933727264404}\n",
      "{'label': 'neutral', 'score': 0.6953954696655273}\n",
      "{'label': 'neutral', 'score': 0.9696556925773621}\n",
      "{'label': 'neutral', 'score': 0.6135007739067078}\n",
      "{'label': 'neutral', 'score': 0.8751550316810608}\n",
      "{'label': 'neutral', 'score': 0.7202498316764832}\n",
      "{'label': 'neutral', 'score': 0.8936662077903748}\n",
      "{'label': 'happiness', 'score': 0.6319570541381836}\n",
      "{'label': 'happiness', 'score': 0.5705071091651917}\n",
      "{'label': 'neutral', 'score': 0.98607337474823}\n",
      "{'label': 'happiness', 'score': 0.7697871923446655}\n",
      "{'label': 'surprise', 'score': 0.5779967308044434}\n",
      "{'label': 'neutral', 'score': 0.5660790801048279}\n",
      "{'label': 'neutral', 'score': 0.9174171090126038}\n",
      "{'label': 'happiness', 'score': 0.8334838151931763}\n",
      "{'label': 'neutral', 'score': 0.8556544780731201}\n",
      "{'label': 'neutral', 'score': 0.9785743951797485}\n",
      "{'label': 'happiness', 'score': 0.9216769933700562}\n",
      "{'label': 'happiness', 'score': 0.6556608080863953}\n",
      "{'label': 'neutral', 'score': 0.9939175248146057}\n",
      "{'label': 'happiness', 'score': 0.7603371143341064}\n",
      "{'label': 'happiness', 'score': 0.9088816046714783}\n",
      "{'label': 'happiness', 'score': 0.9023109674453735}\n",
      "{'label': 'neutral', 'score': 0.5354271531105042}\n",
      "{'label': 'neutral', 'score': 0.8508486747741699}\n",
      "{'label': 'neutral', 'score': 0.9143214821815491}\n",
      "{'label': 'happiness', 'score': 0.7856729626655579}\n",
      "{'label': 'neutral', 'score': 0.9069536924362183}\n",
      "{'label': 'neutral', 'score': 0.7733903527259827}\n",
      "{'label': 'neutral', 'score': 0.9383676648139954}\n",
      "{'label': 'neutral', 'score': 0.9581437706947327}\n",
      "{'label': 'neutral', 'score': 0.984481930732727}\n",
      "{'label': 'neutral', 'score': 0.9238601922988892}\n",
      "{'label': 'happiness', 'score': 0.9007900953292847}\n",
      "{'label': 'happiness', 'score': 0.8047448992729187}\n",
      "{'label': 'neutral', 'score': 0.9239380359649658}\n",
      "{'label': 'neutral', 'score': 0.6645945310592651}\n",
      "{'label': 'neutral', 'score': 0.5378600358963013}\n",
      "{'label': 'neutral', 'score': 0.9879313111305237}\n",
      "{'label': 'neutral', 'score': 0.6198521256446838}\n",
      "{'label': 'happiness', 'score': 0.5384243130683899}\n",
      "{'label': 'neutral', 'score': 0.9833528995513916}\n",
      "{'label': 'neutral', 'score': 0.5837490558624268}\n",
      "{'label': 'neutral', 'score': 0.9369338750839233}\n",
      "{'label': 'neutral', 'score': 0.6761685609817505}\n",
      "{'label': 'happiness', 'score': 0.8995200395584106}\n",
      "{'label': 'neutral', 'score': 0.97654128074646}\n",
      "{'label': 'neutral', 'score': 0.5080356597900391}\n",
      "{'label': 'neutral', 'score': 0.8008676171302795}\n",
      "{'label': 'happiness', 'score': 0.8814610838890076}\n",
      "{'label': 'neutral', 'score': 0.9936591982841492}\n",
      "{'label': 'neutral', 'score': 0.8482085466384888}\n",
      "{'label': 'neutral', 'score': 0.792061448097229}\n",
      "{'label': 'neutral', 'score': 0.9927040338516235}\n",
      "{'label': 'neutral', 'score': 0.6710723042488098}\n",
      "{'label': 'happiness', 'score': 0.9468955993652344}\n",
      "{'label': 'neutral', 'score': 0.9907688498497009}\n",
      "{'label': 'happiness', 'score': 0.5967581272125244}\n",
      "{'label': 'neutral', 'score': 0.9004066586494446}\n",
      "{'label': 'neutral', 'score': 0.5325467586517334}\n",
      "{'label': 'happiness', 'score': 0.8071073293685913}\n",
      "{'label': 'neutral', 'score': 0.7312319874763489}\n",
      "{'label': 'happiness', 'score': 0.8294000625610352}\n",
      "{'label': 'neutral', 'score': 0.5204528570175171}\n",
      "{'label': 'neutral', 'score': 0.4955888092517853}\n",
      "{'label': 'happiness', 'score': 0.5185508131980896}\n",
      "{'label': 'neutral', 'score': 0.7096114158630371}\n",
      "{'label': 'neutral', 'score': 0.9919448494911194}\n",
      "{'label': 'happiness', 'score': 0.5220082998275757}\n",
      "{'label': 'neutral', 'score': 0.9806079864501953}\n",
      "{'label': 'surprise', 'score': 0.9663936495780945}\n",
      "{'label': 'neutral', 'score': 0.8602774739265442}\n",
      "\n",
      "target length:  69\n",
      "test_response length\n",
      "max: 99 \n",
      "min: 19 \n",
      "avg: 69.5234375\n",
      "Hello Huston, here is a reward function\n",
      "{'label': 'neutral', 'score': 0.7127453088760376}\n",
      "{'label': 'happiness', 'score': 0.8178281188011169}\n",
      "{'label': 'neutral', 'score': 0.8172222375869751}\n",
      "{'label': 'neutral', 'score': 0.9539152979850769}\n",
      "{'label': 'happiness', 'score': 0.8373532891273499}\n",
      "{'label': 'neutral', 'score': 0.9851166605949402}\n",
      "{'label': 'neutral', 'score': 0.7360706925392151}\n",
      "{'label': 'happiness', 'score': 0.881365180015564}\n",
      "{'label': 'neutral', 'score': 0.9505987167358398}\n",
      "{'label': 'happiness', 'score': 0.6143799424171448}\n",
      "{'label': 'neutral', 'score': 0.947946310043335}\n",
      "{'label': 'neutral', 'score': 0.9961989521980286}\n",
      "{'label': 'neutral', 'score': 0.9969769716262817}\n",
      "{'label': 'neutral', 'score': 0.5224722623825073}\n",
      "{'label': 'happiness', 'score': 0.867100179195404}\n",
      "{'label': 'neutral', 'score': 0.6936636567115784}\n",
      "{'label': 'neutral', 'score': 0.9938133955001831}\n",
      "{'label': 'neutral', 'score': 0.8405022025108337}\n",
      "{'label': 'happiness', 'score': 0.9251291155815125}\n",
      "{'label': 'neutral', 'score': 0.9771903157234192}\n",
      "{'label': 'neutral', 'score': 0.7178679704666138}\n",
      "{'label': 'neutral', 'score': 0.8880262970924377}\n",
      "{'label': 'neutral', 'score': 0.89857017993927}\n",
      "{'label': 'neutral', 'score': 0.6273374557495117}\n",
      "{'label': 'neutral', 'score': 0.9863807559013367}\n",
      "{'label': 'neutral', 'score': 0.6257737874984741}\n",
      "{'label': 'happiness', 'score': 0.8940134048461914}\n",
      "{'label': 'neutral', 'score': 0.8270902037620544}\n",
      "{'label': 'happiness', 'score': 0.5415453314781189}\n",
      "{'label': 'neutral', 'score': 0.9897630214691162}\n",
      "{'label': 'neutral', 'score': 0.5928057432174683}\n",
      "{'label': 'happiness', 'score': 0.673758864402771}\n",
      "{'label': 'happiness', 'score': 0.8752423524856567}\n",
      "{'label': 'neutral', 'score': 0.9545499682426453}\n",
      "{'label': 'neutral', 'score': 0.9270138144493103}\n",
      "{'label': 'surprise', 'score': 0.7655095458030701}\n",
      "{'label': 'neutral', 'score': 0.8531310558319092}\n",
      "{'label': 'anger', 'score': 0.42093077301979065}\n",
      "{'label': 'neutral', 'score': 0.7099184989929199}\n",
      "{'label': 'neutral', 'score': 0.9862849712371826}\n",
      "{'label': 'surprise', 'score': 0.6146003603935242}\n",
      "{'label': 'neutral', 'score': 0.5754676461219788}\n",
      "{'label': 'neutral', 'score': 0.5390225052833557}\n",
      "{'label': 'neutral', 'score': 0.9911623001098633}\n",
      "{'label': 'happiness', 'score': 0.9391178488731384}\n",
      "{'label': 'neutral', 'score': 0.9677518010139465}\n",
      "{'label': 'happiness', 'score': 0.8120973706245422}\n",
      "{'label': 'neutral', 'score': 0.9867718815803528}\n",
      "{'label': 'neutral', 'score': 0.564675509929657}\n",
      "{'label': 'happiness', 'score': 0.8486809730529785}\n",
      "{'label': 'happiness', 'score': 0.6716611385345459}\n",
      "{'label': 'neutral', 'score': 0.9321475028991699}\n",
      "{'label': 'happiness', 'score': 0.8996338844299316}\n",
      "{'label': 'neutral', 'score': 0.9644034504890442}\n",
      "{'label': 'happiness', 'score': 0.8538888096809387}\n",
      "{'label': 'neutral', 'score': 0.7933173775672913}\n",
      "{'label': 'neutral', 'score': 0.9879313111305237}\n",
      "{'label': 'neutral', 'score': 0.6442779898643494}\n",
      "{'label': 'neutral', 'score': 0.9830532670021057}\n",
      "{'label': 'neutral', 'score': 0.9565465450286865}\n",
      "{'label': 'neutral', 'score': 0.9940807223320007}\n",
      "{'label': 'neutral', 'score': 0.9681925177574158}\n",
      "{'label': 'neutral', 'score': 0.9641845226287842}\n",
      "{'label': 'happiness', 'score': 0.8820887804031372}\n",
      "{'label': 'neutral', 'score': 0.9628612399101257}\n",
      "{'label': 'happiness', 'score': 0.8254244327545166}\n",
      "{'label': 'neutral', 'score': 0.6142300367355347}\n",
      "{'label': 'neutral', 'score': 0.6082223653793335}\n",
      "{'label': 'happiness', 'score': 0.8743118047714233}\n",
      "{'label': 'surprise', 'score': 0.9238083958625793}\n",
      "{'label': 'neutral', 'score': 0.7668336033821106}\n",
      "{'label': 'surprise', 'score': 0.8890531659126282}\n",
      "{'label': 'happiness', 'score': 0.8293323516845703}\n",
      "{'label': 'neutral', 'score': 0.9642207622528076}\n",
      "{'label': 'neutral', 'score': 0.9939790964126587}\n",
      "{'label': 'neutral', 'score': 0.9048190116882324}\n",
      "{'label': 'neutral', 'score': 0.9974604845046997}\n",
      "{'label': 'neutral', 'score': 0.969487726688385}\n",
      "{'label': 'neutral', 'score': 0.7544647455215454}\n",
      "{'label': 'happiness', 'score': 0.8095102310180664}\n",
      "{'label': 'neutral', 'score': 0.7532848715782166}\n",
      "{'label': 'neutral', 'score': 0.993893027305603}\n",
      "{'label': 'neutral', 'score': 0.9917507767677307}\n",
      "{'label': 'happiness', 'score': 0.733300507068634}\n",
      "{'label': 'neutral', 'score': 0.9791086316108704}\n",
      "{'label': 'neutral', 'score': 0.7800416946411133}\n",
      "{'label': 'surprise', 'score': 0.502018392086029}\n",
      "{'label': 'neutral', 'score': 0.5781978964805603}\n",
      "{'label': 'neutral', 'score': 0.9935105443000793}\n",
      "{'label': 'happiness', 'score': 0.9503995776176453}\n",
      "{'label': 'neutral', 'score': 0.9886080026626587}\n",
      "{'label': 'neutral', 'score': 0.7241668701171875}\n",
      "{'label': 'neutral', 'score': 0.9798970818519592}\n",
      "{'label': 'neutral', 'score': 0.9755278825759888}\n",
      "{'label': 'neutral', 'score': 0.8257299065589905}\n",
      "{'label': 'happiness', 'score': 0.9069719314575195}\n",
      "{'label': 'neutral', 'score': 0.8761905431747437}\n",
      "{'label': 'happiness', 'score': 0.8230199813842773}\n",
      "{'label': 'neutral', 'score': 0.9571490287780762}\n",
      "{'label': 'happiness', 'score': 0.8364573121070862}\n",
      "{'label': 'happiness', 'score': 0.8248791098594666}\n",
      "{'label': 'neutral', 'score': 0.9577189683914185}\n",
      "{'label': 'neutral', 'score': 0.4647572934627533}\n",
      "{'label': 'neutral', 'score': 0.985988438129425}\n",
      "{'label': 'happiness', 'score': 0.843737006187439}\n",
      "{'label': 'neutral', 'score': 0.740168035030365}\n",
      "{'label': 'neutral', 'score': 0.9706107378005981}\n",
      "{'label': 'happiness', 'score': 0.8845157027244568}\n",
      "{'label': 'happiness', 'score': 0.9243993759155273}\n",
      "{'label': 'neutral', 'score': 0.9347330927848816}\n",
      "\n",
      "target length:  69\n",
      "test_response length\n",
      "max: 104 \n",
      "min: 13 \n",
      "avg: 69.22727272727273\n",
      "<class 'list'>\n",
      "[0.043160421507699144, 0.7820284366607666, 0.010183337487672504, 0.40272996975825387, 0.9092021981875101, 1.0288910071055093, 0.4315500096841292, 0.5838782997692332, 0.015453594071524484, 0.04187462546608665, 0.4091586338149176, 0.8126887679100037, 0.017917770605820876, 1.3979418079058328, 0.7549991210301716, 0.1364784836769104, 0.182820715402302, 0.2895102330616542, 0.4157310983409052, 0.05599474906921387, 0.9288055896759033, 1.6261796951293945, 0.021615179379781087, 0.21896031995614368, 0.6963410377502441, 0.0231210192044576, 0.38769503434499103, 0.12997940453616055, 0.7010491689046223, 0.01489180326461792, 0.5742762486139933, 2.39543875058492, 2.46312215924263, 2.351074367761612, 0.3779253363609314, 0.008092142068422759, 0.03644603490829468, 1.424101505960737, 0.09945926666259766, 0.5040748302753155, 0.033354622125625615, 0.001971370644039578, 0.00031669472539147666, 0.5867344842237585, 0.8987424048510465, 0.4793928219721868, 2.471080869436264, 0.04558645685513814, 2.0534406105677285, 0.01095426082611084, 1.6452458500862122, 1.7738831043243408, 0.7755080678246238, 0.050663550694783524, 1.0734829637739394, 0.05839889390127999, 0.571306308110555, 1.5665773153305054, 0.06877555165972028, 0.8998328989202327, 0.34699782729148865, 0.4233776745588883, 1.7242926359176636, 0.8885965347290039, 0.12046998076968723, 1.8531346321105957, 0.25255794587888214, 0.4994591286307887, 0.22510114312171936, 0.5898445182376437, 0.018552660942077637, 0.045745168413434706, 1.298826734224955, 2.7796940008799234, 0.5282038052876791, 0.33150218684097815, 1.3976739985602242, 0.32755841811498004, 0.5668465907757099, 1.3185468741825648, 1.227981224656105, 0.5485188961029053, 0.046003540356953934, 0.0959160327911377, 0.5992943474224636, 1.2048572301864624, 1.4062690734863281, 0.23450034506180706, 0.433494932949543, 0.6017187237739563, 0.6043865531682968, 0.0317554235458374, 2.6603148380915322, 0.002494745784335666, 0.9090167880058289, 1.7176569998264313, 0.08224473396937051, 4.812019467353821, 0.3093009442090988, 0.5066626932885911, 1.6278030474980671, 0.326846221397663, 0.05844073636191231, 0.9621313810348511, 1.2084154287974038, 2.471080869436264, 0.9338551759719849, 2.486102730035782, 0.8785569071769714, 0.5439531803131104, 0.9390421424593244, 0.706844585282462, 0.9767960177527533, 0.039238381385803225, 0.012734069543726304, 0.02304442904212258, 0.02110006014506022, 0.00942532763336644, 0.04070626063780351, 0.4068724487138831, 0.00861024542858726, 2.3482302824656167, 0.8973834189501676, 0.0006540380418300629, 0.4978508949279785, 3.1692083676656084, 0.2978298840699372, 4.868292808532715]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "rewards: list = [reward(batch) for batch in DataLoader(test_data, batch_size=128)]\n",
    "print(type(rewards[0])) # should be a list\n",
    "print(rewards[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_length = 69\n",
    "# def calculate_emotion_score(response_sentiment: str, correct_emotion: str) -> float:\n",
    "#     # correct: save the score from analyser \n",
    "#     # wrong: [TO-DO] (save 1 - score from analyser )\n",
    "#     # emotion_output = analyser(response)[0]\n",
    "#     emotion_output = response_sentiment\n",
    "\n",
    "#     if emotion_output[\"label\"] == correct_emotion:\n",
    "#         emotion_score = emotion_output[\"score\"] * 10\n",
    "#     else:\n",
    "#         emotion_score = 1 - emotion_output[\"score\"]\n",
    "#     return emotion_score\n",
    "\n",
    "# def calculate_length_score(response: str) -> float:\n",
    "#     # print(\"len: \", len(response), \"res: \", response)\n",
    "#     # use reciprocal of length difference to calculate\n",
    "#     # the larger the difference the smaller the score is\n",
    "#     length_diff = abs(len(response) - target_length)\n",
    "#     # print(length_diff)\n",
    "#     length_score = 1 / (length_diff + 1)\n",
    "#     return length_score\n",
    "# import statistics\n",
    "# def reward(batch: dict) -> list:\n",
    "#     print(\"Hello Huston, here is a reward function\")\n",
    "#     # correct_emotion = batch['label']\n",
    "#     # print(batch)\n",
    "#     # print(correct_emotion)\n",
    "#     rewards = []\n",
    "#     res_len = []\n",
    "#     for response in batch:\n",
    "#         res_len.append(len(response['test_response']))\n",
    "#         # print(response['test_response'])\n",
    "#         # print(response['test_response_sentiment'])\n",
    "#         # print(emotion_labels[response['label']], \"\\n\")\n",
    "#         correct_emotion = emotion_labels[response['label']]\n",
    "#         emotion_score = calculate_emotion_score(response['test_response_sentiment'], correct_emotion)\n",
    "#         length_score = calculate_length_score(response['test_response'])\n",
    "#         # use the product of two score as reward\n",
    "#         reward_product = emotion_score * length_score\n",
    "#         rewards.append(reward_product)\n",
    "#     print(\"\\ntarget length: \", target_length)\n",
    "#     print(\"test_response length\")\n",
    "#     print(\"max:\", max(res_len),\"\\nmin:\", min(res_len),\"\\navg:\", statistics.mean(res_len))\n",
    "    \n",
    "#     return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_length = 69\n",
    "# def calculate_emotion_score(response: str, correct_emotion: str) -> float:\n",
    "#     # correct: save the score from analyser \n",
    "#     # wrong: [TO-DO] (save 1 - score from analyser )\n",
    "#     emotion_output = analyser(response)[0]\n",
    "#     print(emotion_output)\n",
    "#     if emotion_output[\"label\"] == correct_emotion:\n",
    "#         emotion_score = emotion_output[\"score\"] * 10\n",
    "#     else:\n",
    "#         emotion_score = 1 - emotion_output[\"score\"]\n",
    "#     return emotion_score\n",
    "\n",
    "# def calculate_length_score(response: str) -> float:\n",
    "#     # print(\"len: \", len(response), \"res: \", response)\n",
    "#     # use reciprocal of length difference to calculate\n",
    "#     # the larger the difference the smaller the score is\n",
    "#     length_diff = abs(len(response) - target_length)\n",
    "#     # print(length_diff)\n",
    "#     length_score = 1 / (length_diff + 1)\n",
    "#     return length_score\n",
    "# import statistics\n",
    "# def reward(batch: dict) -> list:\n",
    "#     print(\"Hello Huston, here is a reward function\")\n",
    "#     # correct_emotion = batch['label']\n",
    "#     # print(batch)\n",
    "#     # correct_emotion = emotion_labels[batch['label']]\n",
    "#     # print(correct_emotion)\n",
    "#     rewards = []\n",
    "#     res_len = []\n",
    "#     for response, raw_correct_emotion in zip(batch[\"response\"], batch[\"label\"]):\n",
    "#         # print(response, \"here\")\n",
    "#         correct_emotion = emotion_labels[raw_correct_emotion]\n",
    "#         res_len.append(len(response))\n",
    "#         # print(response['test_response'])\n",
    "#         # print(response['test_response_sentiment'])\n",
    "#         # print(emotion_labels[response['label']], \"\\n\")\n",
    "        \n",
    "#         emotion_score = calculate_emotion_score(response, correct_emotion)\n",
    "#         length_score = calculate_length_score(response)\n",
    "#         # use the product of two score as reward\n",
    "#         reward_product = emotion_score * length_score\n",
    "#         rewards.append(reward_product)\n",
    "#     print(\"\\ntarget length: \", target_length)\n",
    "#     print(\"test_response length\")\n",
    "#     print(\"max:\", max(res_len),\"\\nmin:\", min(res_len),\"\\navg:\", statistics.mean(res_len))\n",
    "    \n",
    "#     return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for response, raw_correct_emotion in zip(batch[\"input_ids\"], batch[\"label\"]):\n",
    "        # print(response, \"here\")\n",
    "        correct_emotion = emotion_labels[raw_correct_emotion]\n",
    "        print(correct_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wards = reward(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock_batch = {\n",
    "#     \"query\": [\n",
    "#         {\"content\": {\"text\": \"Hello, how are you?\", \"emotion\": \"sadness\"}},\n",
    "#         {\"content\": {\"text\": \"It's a nice day today.\", \"emotion\": \"neutral\"}},\n",
    "#         {\"content\": {\"text\": \"\", \"emotion\": \"anger\"}}\n",
    "#     ],\n",
    "#     \"response\": [\n",
    "#         \"I am doing well, thank you!\",\n",
    "#         \"Yes, it is a beautiful day.\",\n",
    "#         \"Man, wtf was that???? OK, I'm leaving now! Go fuck yourself and your bullshit.\"\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# rewards = reward(mock_batch)\n",
    "# print(\"Rewards:\", rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rewards = reward(result)\n",
    "# print(\"\\nRewards len:\", len(rewards))\n",
    "# print(\"Rewards:\", rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup Tuner\n",
    "# tuner = PPOTrainer(\n",
    "# \tconfig=ppo_config,\n",
    "# \tmodel=base_model_with_adapter,\n",
    "# \ttokenizer=tokenizer,\n",
    "# \tdataset=dataset,\n",
    "# \toptimizer=optimizer,\n",
    "# \tlr_scheduler=lr_scheduler\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in trange(1, colour=\"blue\"):\n",
    "# \tfor batch in tqdm(tuner.dataloader, colour=\"yellow\"):\n",
    "# \t\tquery_tensors = batch[\"input_ids\"] # somehow has 2048 ids\n",
    "# \t\tbreak\n",
    "# \t\tresponse_tensors = tuner.generate(\n",
    "# \t\t\tquery_tensors,\n",
    "# \t\t\treturn_prompt=False,\n",
    "# \t\t\tbatch_size=1,   # must set to 1 if using streamer\n",
    "# \t\t\tstreamer=streamer,  # use streamer to show the generation process\n",
    "# \t\t\t**generation_config.to_dict()\n",
    "# \t\t)\n",
    "# \t\tbatch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "# \t\tresponse_tensors = [torch.LongTensor(t.to(\"cpu\")) for t in response_tensors]\n",
    "\n",
    "# \t\treward_scores = reward(batch)\n",
    "# \t\trewards = [torch.FloatTensor(torch.tensor(scores, device=\"cpu\")) for scores in reward_scores]\n",
    "\n",
    "# \t\tstats = tuner.step(query_tensors, response_tensors, rewards)\n",
    "# \t\ttuner.log_stats(stats, batch, rewards)\n",
    "\n",
    "# # model_artifact = wandb.Artifact(\n",
    "# # \twandb.config[\"fine_tuned_model\"],\n",
    "# # \ttype=\"model\"\n",
    "# # )\n",
    "\n",
    "# # tuner.model = torch.compile(tuner.model)\n",
    "# # tuner.model.push_to_hub(repo_id=\"response_generator_for_emotion_chat_bot\", commit=\"\", create_pr=True)\n",
    "# # with tempfile.TemporaryDirectory() as temp_dir:\n",
    "# # \ttuner.model.save_pretrained(temp_dir, save_embedding_layers=True)\n",
    "# # \tmodel_artifact.add_dir(temp_dir)\n",
    "# # \trun.log_artifact(model_artifact)\n",
    "\n",
    "# wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat-bot-20tW9agt-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
